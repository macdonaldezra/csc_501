{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Sentiment for MovieLens Tags\n",
    "\n",
    "The Hugging Face Model can be found [here](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base) that was used to compute emotions in the ensuing notebook cells for the tag strings found in the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import typing as T\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Link to model: https://huggingface.co/j-hartmann/emotion-english-distilroberta-base\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'anger', 'score': 0.029231058433651924},\n",
       "  {'label': 'disgust', 'score': 0.22560150921344757},\n",
       "  {'label': 'fear', 'score': 0.00604574428871274},\n",
       "  {'label': 'joy', 'score': 0.00475974241271615},\n",
       "  {'label': 'neutral', 'score': 0.628390371799469},\n",
       "  {'label': 'sadness', 'score': 0.059987958520650864},\n",
       "  {'label': 'surprise', 'score': 0.04598362371325493}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"some text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "One row was dropped from the dataset that contained an empty string to reduce the likelihood of running into edge case issues while computing different statistics on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df: 1108997\n",
      "Number of tags: 74714\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path().cwd().parent / \"data\"\n",
    "df = pd.read_csv(DATA_PATH / \"ml-latest\" / \"tags.csv\")\n",
    "print(f\"Length of df: {len(df)}\")\n",
    "df.dropna(inplace=True)\n",
    "tags = df[\"tag\"].unique()\n",
    "\n",
    "print(f\"Number of unique tags: {len(tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying 74714 tags.\n",
      "Function compute_score Took 3958.4121 seconds.\n"
     ]
    }
   ],
   "source": [
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def timeit_wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Function {func.__name__} Took {total_time:.4f} seconds.\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    return timeit_wrapper\n",
    "\n",
    "@timeit\n",
    "def get_scores(classifier: T.Any, tags: T.List[str]) -> T.Tuple[T.Union[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute a classification score for each tag.\n",
    "    \"\"\"\n",
    "    print(f\"Classifying {len(tags)} tags.\")\n",
    "    predictions = []\n",
    "    for i in range(0, len(tags), 1000):\n",
    "        predictions.append()\n",
    "    predictions = classifier(tags)\n",
    "    output = []\n",
    "\n",
    "@timeit\n",
    "def compute_score(classifier: T.Any, tags: T.List[str]) -> T.Tuple[T.Union[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute a classification score for each tag in the provided tag list.\n",
    "    \"\"\"\n",
    "    print(f\"Classifying {len(tags)} tags.\")\n",
    "    predictions = classifier(tags)\n",
    "    output = []\n",
    "    for tag, prediction in zip(tags, predictions):\n",
    "        max_pred = max(prediction, key=lambda x:x[\"score\"])\n",
    "        output.append((tag, max_pred[\"label\"], max_pred[\"score\"]))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "tag_tuples = compute_score(classifier, tags.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>epic</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.406139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medieval</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.795388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sci-fi</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.441692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>space action</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.595582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imdb top 250</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.629384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tag   emotion     score\n",
       "0          epic  surprise  0.406139\n",
       "1      Medieval   neutral  0.795388\n",
       "2        sci-fi   neutral  0.441692\n",
       "3  space action   neutral  0.595582\n",
       "4  imdb top 250   neutral  0.629384"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out = pd.DataFrame(tag_tuples, columns=[\"tag\", \"emotion\", \"score\"])\n",
    "df_out.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv(DATA_PATH / \"ml-latest\" / \"computed-tags.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4d6ec531e2a64382ba76cafd54991a0cad84c5a8d53126302b25d3c38b8eed8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
